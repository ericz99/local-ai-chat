# Local Multi-Modal Chat CLI (LMCC)

A privacy-focused, locally-run chat interface supporting multi-modal interactions with AI models. Built with Python and Ollama.

## Features

- ğŸ”’ **100% Local Execution** - No data leaves your machine
- ğŸ–¼ï¸ **Multi-Modal Support** - Chat with images, documents, and text
- âš¡ **Real-time Streaming** - Typewriter-style responses
- ğŸ§  **Model Management** - Easily switch between local LLMs
- ğŸ”„ **Context Aware** - Maintains conversation history
- ğŸ›¡ï¸ **Encrypted History** - Secures chat sessions
- ğŸ¨ **Rich CLI Interface** - Beautiful terminal formatting

## Installation

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai/) installed locally
- Supported models downloaded (e.g., `llava`, `mistral`)

```bash
# Clone repository
git clone https://github.com/ericz99/local-ai-chat.git
cd local-ai-chat

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Install dependencies
pip install -e .

# Make script executable
chmod +x setup_env.sh

# Run setup
./scripts/setup_env.sh
```

## Configuration

```bash
# data/config/settings.yaml

model:
  default: "llava:latest"
  temperature: 0.7  # 0-1, creativity control
  gpu_layers: 20    # Use 0 for CPU-only
  max_history: 6    # Context window size

privacy:
  encrypt_history: true
  auto_clear_temp: true

ui:
  theme: "dark"     # dark/light
  response_color: "cyan"
```
